# âœ‹ Hand Gesture Recognition using CNN + PyTorch

This repository implements a **multi-class hand gesture classification system** using deep learning. The model is trained on the **LeapGestRecog** dataset and leverages a **Convolutional Neural Network (CNN)** built using **PyTorch**. It also includes feature-space visualization using **UMAP**, evaluation metrics, and confidence-based predictions.

ğŸ“Œ **Task Objective:**  
Classify grayscale hand gesture images into one of 10 categories using a custom CNN trained from scratch.

---

## ğŸ”§ Features

- ğŸ”¹ Custom PyTorch Dataset class for LeapGestRecog
- ğŸ”¹ CNN model trained on gesture images
- ğŸ”¹ Train-test split with PyTorch `DataLoader`
- ğŸ”¹ Real-time accuracy and loss tracking
- ğŸ”¹ Evaluation with confusion matrix, classification report
- ğŸ”¹ Visualization of:
  - UMAP feature embeddings
  - Per-class accuracy
  - Misclassified images
  - Prediction with confidence scores

---

## ğŸ“‚ Dataset

- **Source:** [LeapGestRecog â€“ Kaggle Dataset](https://www.kaggle.com/datasets/gti-upm/leapgestrecog)
- **Structure:**  
  `/leapGestRecog/00/palm/`  
  `/leapGestRecog/00/thumb/`  
  ...  
  `/leapGestRecog/09/ok/`

- **Images:**  
  - 200 Ã— 10 Ã— 10 = **~20,000 grayscale images**  
  - Size: 64Ã—64 (resized from original)
  - Format: `.png`  
  - Classes: `10` hand gestures (e.g., palm, L, fist, index, ok...)

---

## ğŸ› ï¸ Workflow

1. **Data Preprocessing** ğŸ§¹  
   - Images resized to 64Ã—64 grayscale  
   - Normalized to [-1, 1]  
   - Labels extracted from folder names  
   - Train-test split using `train_test_split`

2. **Model Architecture** ğŸ§   
   - Custom CNN:
     - 3 Conv layers + ReLU + MaxPool  
     - Fully connected classifier
   - Output layer with 10 neurons (Softmax)

3. **Training & Evaluation** ğŸ”  
   - Optimizer: Adam  
   - Loss Function: CrossEntropy  
   - Epochs: 15  
   - Accuracy and loss tracked per epoch  
   - UMAP applied on CNN features

4. **Visualization & Interpretation** ğŸ“Š  
   - Confusion Matrix Heatmap  
   - Loss & Accuracy curves  
   - UMAP 2D projection  
   - Per-class performance plots  
   - Predicted images with confidence

---

## ğŸ§  Methodology

- **Transformations**  
  - Resize to `(64Ã—64)`  
  - `ToTensor` + `Normalize` with mean=0.5, std=0.5  

- **Model Training**  
  - `train()` and `evaluate()` loops  
  - Accuracy and loss captured per epoch

- **Prediction Confidence**  
  - Softmax used to extract probabilities  
  - Top class shown with confidence %

- **Feature Visualization**  
  - `umap-learn` used on CNN features for 2D embedding
  - Shows clustering per gesture class

---

## âœ… Results

<div align="center">

<table>
<tr>
<td>

ğŸ“‹ **Classification Report:**

| Metric    | palm | L   | fist | ok  | ... | avg  |
|-----------|------|-----|------|-----|-----|------|
| Precision | 0.98 | 0.97| 0.96 | 0.99| ... | 0.98 |
| Recall    | 0.97 | 0.98| 0.95 | 0.99| ... | 0.98 |
| F1-Score  | 0.97 | 0.97| 0.95 | 0.99| ... | 0.98 |
| Support   | 200  | 200 | 200  | 200 | ... | â€”    |

</td>

<td style="padding-left: 40px;">

ğŸ“Š **Summary Metrics:**

| Metric               | Value     |
|----------------------|-----------|
| ğŸ¯ Accuracy           | **98.25%** |
| ğŸ“Š Macro Precision    | **0.981**  |
| ğŸ“ˆ Macro Recall       | **0.982**  |
| ğŸ“‰ Macro F1-Score     | **0.980**  |

</td>
</tr>
</table>

</div>


ğŸ§  The CNN achieved high accuracy across all gesture classes, with strong confidence and minimal misclassification.

---

## ğŸ“¸ Visual Outputs

<div align="center">

<table>
  <tr>
    <td>
      <strong>ğŸ“Š Number of Samples per Class</strong><br>
      <img src="Images/number_of_sample_per_class.png" width="400">
    </td>
    <td>
      <strong>ğŸ“Š Confusion Matrix</strong><br>
      <img src="Images/confusion_matrix.png" width="400">
    </td>
  </tr>
  <tr>
    <td>
      <strong>ğŸ“ˆ Accuracy Curve</strong><br>
      <img src="Images/accuracy_curve.png" width="400">
    </td>
    <td>
      <strong>ğŸ“‰ Loss Curve</strong><br>
      <img src="Images/loss_curve.png" width="400">
    </td>
  </tr>
  <tr>
    <td>
      <strong>ğŸ¯ Per-Class Accuracy</strong><br>
      <img src="Images/per_class_accuracy.png" width="400">
    </td>
    <td>
      <strong>ğŸ“‹ Precision / Recall / F1-Score per Class</strong><br>
      <img src="Images/per_class_precision_recall_f1.png" width="400">
    </td>
  </tr>
  <tr>
    <td>
      <strong>ğŸŒ UMAP Feature Embedding</strong><br>
      <img src="Images/Umap.png" width="400">
    </td>
    <td>
      <strong>ğŸ”¥ Activation Heatmap (Feature Map)</strong><br>
      <img src="Images/hitmap.png" width="400">
    </td>
  </tr>
  <tr>
    <td>
      <strong>ğŸ” Prediction Example 1</strong><br>
      <img src="Images/prediction_1.png" width="400">
    </td>
    <td>
      <strong>ğŸ” Prediction Example 2</strong><br>
      <img src="Images/prediction_2.png" width="400">
    </td>
  </tr>

</table>

</div>


---

## ğŸš€ Getting Started

1. Clone the repository  
   ```bash
   git clone https://github.com/shreyakmukherjee/leap-gesture-cnn.git  
   cd leap-gesture-cnn
2. Install dependencies
   ```bash
   pip install -r requirements.txt
3. Run the notebook
Open leap-gesture-recognition.ipynb in Jupyter or Colab.

ğŸ§ª Custom Prediction
To visualize random predictions with confidence:

---


---

## ğŸ“Œ Dependencies

- numpy  
- matplotlib  
- seaborn  
- pandas  
- scikit-learn  
- torch  
- torchvision  
- umap-learn  
- tqdm  

---

## ğŸ§  Algorithm Used

- Custom CNN built with PyTorch  
- Softmax for confidence scores  
- UMAP for feature visualization  

---

## ğŸ“Œ License

This project is licensed under the **MIT License** â€“ see the LICENSE file for details.

---

## âœï¸ Author

ğŸ‘¤ **Shreyak Mukherjee**  
ğŸ“‚ GitHub: [shreyakmukherjee](https://github.com/shreyakmukherjee)  
ğŸ”— LinkedIn: [https://www.linkedin.com/in/shreyak-mukherjee-203558275/](https://www.linkedin.com/in/shreyak-mukherjee-203558275/)



---

## ğŸ“¥ Next Steps

- Rename images (e.g., `confusion_matrix.png`, `umap_projection.png`) and store them in a `/Images/` folder inside the repo.
- Save this content as `README.md` in the root directory.
- Push to GitHub and you're done!

Would you like help turning this into a **template for future projects**, or adding **badges**, `requirements.txt`, or GitHub actions?
----
